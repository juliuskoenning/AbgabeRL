{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train of Tetris Reinforcement Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is inspired by the following project: https://github.com/uvipen/Tetris-deep-Q-learning-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "from collections import deque\n",
    "from random import randint, random, sample\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tensorboardX import SummaryWriter\n",
    "from src.deep_q_network import DeepQNetwork\n",
    "from src.tetris import Tetris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a class to hold options/configuration\n",
    "class Options:\n",
    "    def __init__(self):\n",
    "        # game parameters\n",
    "        self.width = 10\n",
    "        self.height = 20\n",
    "        self.block_size = 30\n",
    "        # learning parameters\n",
    "        self.batch_size = 512\n",
    "        self.lr = 1e-3\n",
    "        self.gamma = 0.99\n",
    "        self.initial_epsilon = 1\n",
    "        self.final_epsilon = 1e-3\n",
    "        self.num_decay_epochs = 2000\n",
    "        self.num_epochs = 3000\n",
    "        self.save_interval = 1000\n",
    "        self.replay_memory_size = 30000\n",
    "        # paths for saving logs and models\n",
    "        self.log_path = \"tensorboard\"\n",
    "        self.saved_path = \"trained_models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance of options\n",
    "opt = Options()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize variables\n",
    "total_reward = 0.0\n",
    "total_episodes = 0\n",
    "max_score = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the seed for torch\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(123)\n",
    "else:\n",
    "    torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the log directory\n",
    "if os.path.isdir(opt.log_path):\n",
    "    shutil.rmtree(opt.log_path)\n",
    "os.makedirs(opt.log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a SummaryWriter for tensorboard\n",
    "writer = SummaryWriter(opt.log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the Tetris environment\n",
    "env = Tetris(width=opt.width, height=opt.height, block_size=opt.block_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the DeepQNetwork model\n",
    "model = DeepQNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=opt.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the loss function\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the game environment\n",
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if cuda is available, move model and state to GPU\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "    state = state.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize replay memory\n",
    "replay_memory = deque(maxlen=opt.replay_memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start training epochs (this wasn't executed again prior to pushing, training took two days)\n",
    "epoch = 0\n",
    "while epoch < opt.num_epochs:\n",
    "    # get the next possible states\n",
    "    next_steps = env.get_next_states()\n",
    "    \n",
    "    # determine if the agent should explore or exploit\n",
    "    epsilon = opt.final_epsilon + (max(opt.num_decay_epochs - epoch, 0) * (\n",
    "            opt.initial_epsilon - opt.final_epsilon) / opt.num_decay_epochs)\n",
    "    u = random()\n",
    "    random_action = u <= epsilon\n",
    "    \n",
    "    # extract next possible actions and their states\n",
    "    next_actions, next_states = zip(*next_steps.items())\n",
    "    next_states = torch.stack(next_states)\n",
    "    \n",
    "    # if cuda is available, move the next_states to GPU\n",
    "    if torch.cuda.is_available():\n",
    "        next_states = next_states.cuda()\n",
    "\n",
    "    # switch to eval mode for prediction\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(next_states)[:, 0]\n",
    "    model.train()\n",
    "    \n",
    "    # choose action\n",
    "    if random_action:\n",
    "        index = randint(0, len(next_steps) - 1)\n",
    "    else:\n",
    "        index = torch.argmax(predictions).item()\n",
    "\n",
    "    next_state = next_states[index, :]\n",
    "    action = next_actions[index]\n",
    "    \n",
    "    # make the chosen action, get reward and check if game is done\n",
    "    reward, done = env.step(action, render=True)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        next_state = next_state.cuda()\n",
    "\n",
    "    # append the experience to the replay memory\n",
    "    replay_memory.append([state, reward, next_state, done])\n",
    "    \n",
    "    if done:\n",
    "        # if the game is done, reset the game environment and save the score\n",
    "        final_score = env.score\n",
    "        final_tetrominoes = env.tetrominoes\n",
    "        final_cleared_lines = env.cleared_lines\n",
    "        state = env.reset()\n",
    "        if torch.cuda.is_available():\n",
    "            state = state.cuda()\n",
    "    else:\n",
    "        state = next_state\n",
    "        continue\n",
    "\n",
    "    # wait until replay memory is sufficiently full\n",
    "    if len(replay_memory) < opt.replay_memory_size / 10:\n",
    "        continue\n",
    "\n",
    "    # calculate and print average reward\n",
    "    total_reward += final_score\n",
    "    total_episodes += 1\n",
    "    average_reward = total_reward / total_episodes\n",
    "    print(\"Average reward: {}\".format(average_reward))\n",
    "\n",
    "    # log the rewards\n",
    "    writer.add_scalar('Train/Average Reward', average_reward, epoch - 1)\n",
    "    \n",
    "    # calculate and print max score\n",
    "    max_score = max(max_score, final_score)\n",
    "    print(\"Maximal achieved score: {}\".format(max_score))\n",
    "    writer.add_scalar('Train/Max Score', max_score, epoch - 1)\n",
    "    \n",
    "    # increment the epoch\n",
    "    epoch += 1\n",
    "    \n",
    "    # sample a batch from the replay memory\n",
    "    batch = sample(replay_memory, min(len(replay_memory), opt.batch_size))\n",
    "    state_batch, reward_batch, next_state_batch, done_batch = zip(*batch)\n",
    "    state_batch = torch.stack(tuple(state for state in state_batch))\n",
    "    reward_batch = torch.from_numpy(np.array(reward_batch, dtype=np.float32)[:, None])\n",
    "    next_state_batch = torch.stack(tuple(state for state in next_state_batch))\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        state_batch = state_batch.cuda()\n",
    "        reward_batch = reward_batch.cuda()\n",
    "        next_state_batch = next_state_batch.cuda()\n",
    "\n",
    "    # compute the q values\n",
    "    q_values = model(state_batch)\n",
    "    \n",
    "    # switch to eval mode for prediction\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        next_prediction_batch = model(next_state_batch)\n",
    "    model.train()\n",
    "    \n",
    "    # compute the target q values\n",
    "    y_batch = torch.cat(\n",
    "        tuple(reward if done else reward + opt.gamma * prediction for reward, done, prediction in\n",
    "                zip(reward_batch, done_batch, next_prediction_batch)))[:, None]\n",
    "\n",
    "    # compute the loss, perform backpropagation, and update the weights\n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(q_values, y_batch)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # print the training information\n",
    "    print(\"Epoch: {}/{}, Action: {}, Score: {}, Tetrominoes {}, Cleared lines: {}\".format(\n",
    "        epoch,\n",
    "        opt.num_epochs,\n",
    "        action,\n",
    "        final_score,\n",
    "        final_tetrominoes,\n",
    "        final_cleared_lines))\n",
    "    writer.add_scalar('Train/Score', final_score, epoch - 1)\n",
    "    writer.add_scalar('Train/Tetrominoes', final_tetrominoes, epoch - 1)\n",
    "    writer.add_scalar('Train/Cleared lines', final_cleared_lines, epoch - 1)\n",
    "    writer.add_scalar('Train/Loss', loss.item(), epoch - 1)\n",
    "    writer.add_scalar('Train/Epsilon', epsilon, epoch - 1)\n",
    "\n",
    "    # save the model at certain intervals\n",
    "    if epoch > 0 and epoch % opt.save_interval == 0:\n",
    "        torch.save(model, \"{}/tetris_{}\".format(opt.saved_path, epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the final model\n",
    "torch.save(model, \"{}/tetris\".format(opt.saved_path))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "moon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d7f375fe7c3024b2039c128ab0d6d39d3322a41f24aad03cfdc19378924b5e1e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
